{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3ae631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5eb9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f94de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp39-none-macosx_10_9_x86_64.whl (120.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp39-cp39-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch==1.8.0) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch==1.8.0) (4.3.0)\n",
      "Requirement already satisfied: tqdm in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.9.0) (4.64.1)\n",
      "Requirement already satisfied: requests in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.9.0) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (3.3)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.1 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\n",
      "torchaudio 2.0.1 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0 torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8aad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (1.8.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.0-cp39-none-macosx_10_9_x86_64.whl (139.8 MB)\n",
      "Requirement already satisfied: jinja2 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kartikeyasyal/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.9.0 requires torch==1.8.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41404d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 6323.74657, Precision: 0.067, Recall: 0.060, F1 Score: 0.063\n",
      "Epoch 2: Train Loss: 2971.23472, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 3: Train Loss: 1608.57225, Precision: 0.066, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 4: Train Loss: 904.71808, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 5: Train Loss: 509.40945, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 6: Train Loss: 336.30222, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 7: Train Loss: 254.87348, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 8: Train Loss: 215.39131, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 9: Train Loss: 284.37480, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 10: Train Loss: 269.56684, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 11: Train Loss: 210.65988, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 12: Train Loss: 108.75902, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 13: Train Loss: 38.92789, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 14: Train Loss: 29.58287, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 15: Train Loss: 11.81076, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 16: Train Loss: 6.47649, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 17: Train Loss: 6.50534, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 18: Train Loss: 5.08135, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 19: Train Loss: 4.75514, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 20: Train Loss: 3.75534, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 21: Train Loss: 3.34722, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 22: Train Loss: 3.10228, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 23: Train Loss: 2.94775, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 24: Train Loss: 2.82706, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 25: Train Loss: 2.71446, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 26: Train Loss: 2.61597, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 27: Train Loss: 2.52790, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 28: Train Loss: 2.44766, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 29: Train Loss: 2.37357, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 30: Train Loss: 2.30434, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 31: Train Loss: 2.23918, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 32: Train Loss: 2.17742, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 33: Train Loss: 2.11869, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 34: Train Loss: 2.06268, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 35: Train Loss: 2.00912, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 36: Train Loss: 1.95767, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 37: Train Loss: 1.90785, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 38: Train Loss: 1.85912, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 39: Train Loss: 1.81102, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 40: Train Loss: 1.76322, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 41: Train Loss: 1.71544, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 42: Train Loss: 1.66757, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 43: Train Loss: 1.61984, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 44: Train Loss: 1.57293, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 45: Train Loss: 1.52731, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 46: Train Loss: 1.48354, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 47: Train Loss: 1.44260, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 48: Train Loss: 1.40502, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 49: Train Loss: 1.37190, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n",
      "Epoch 50: Train Loss: 1.34289, Precision: 0.067, Recall: 0.063, F1 Score: 0.065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.33\n",
    "OUTPUT_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 50\n",
    "\n",
    "# Define the model\n",
    "class BLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tagset_size):\n",
    "        super(BLSTM, self).__init__()\n",
    "        self.embedding_dim = EMBEDDING_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.dropout = DROPOUT\n",
    "        self.output_dim = OUTPUT_DIM\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.blstm = nn.LSTM(self.embedding_dim, self.hidden_dim, \n",
    "                             num_layers=self.num_layers, bidirectional=True,\n",
    "                             dropout=self.dropout)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim*2, self.output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.tag_projection = nn.Linear(self.output_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.blstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        elu_out = self.elu(tag_space)\n",
    "        tag_scores = self.tag_projection(elu_out)\n",
    "        return tag_scores\n",
    "    \n",
    "def train(model, optimizer, loss_function, train_data, dev_data):\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        model.train()\n",
    "        for sentence, tags in train_data:\n",
    "            model.zero_grad()\n",
    "            sentence = autograd.Variable(torch.LongTensor(sentence))\n",
    "            targets = autograd.Variable(torch.LongTensor(tags))\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            num_correct += (predicted_tags == targets).sum().item()\n",
    "            num_total += len(targets)\n",
    "\n",
    "        precision, recall, f1 = evaluate(model, dev_data)\n",
    "        print(\"Epoch {}: Train Loss: {:.5f}, Precision: {:.3f}, Recall: {:.3f}, F1 Score: {:.3f}\" .format(epoch+1, train_loss, precision, recall, f1))\n",
    "\n",
    "torch.save(model.state_dict(), 'blstm1.pt')\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in data:\n",
    "            sentence = autograd.Variable(torch.LongTensor(sentence))\n",
    "            targets = autograd.Variable(torch.LongTensor(tags))\n",
    "            tag_scores = model(sentence)\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            num_correct += (predicted_tags == targets).sum().item()\n",
    "            num_total += len(targets)\n",
    "            correct += (predicted_tags == targets).sum().item() * 1.0 / len(targets)\n",
    "\n",
    "    precision = correct / num_correct\n",
    "    recall = correct / num_total\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Read in the train and dev data\n",
    "def read_data(file_path, has_tags=True):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(file_path) as f:\n",
    "        sentence = []\n",
    "        tags_sequence = []\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tags_sequence)\n",
    "                sentence = []\n",
    "                tags_sequence = []\n",
    "            else:\n",
    "                line = line.strip().split()\n",
    "                word = line[1]\n",
    "                sentence.append(word)\n",
    "                if has_tags:\n",
    "                    tag = line[2]\n",
    "                    tags_sequence.append(tag)\n",
    "    return sentences, tags\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths to the train and dev data files\n",
    "train_file_path = \"train.txt\"\n",
    "dev_file_path = \"dev.txt\"\n",
    "test_file_path = \"test.txt\"\n",
    "\n",
    "\n",
    "# Read in the train and dev data\n",
    "train_sentences, train_tags = read_data(train_file_path)\n",
    "dev_sentences, dev_tags = read_data(dev_file_path)\n",
    "test_sentences, _ = read_data(test_file_path, has_tags=False)\n",
    "\n",
    "# Create the vocabulary and tag sets\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "for sentence, tags in zip(train_sentences, train_tags):\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# Add the unknown token to the vocabulary\n",
    "word_to_ix[\"<UNK>\"] = len(word_to_ix)\n",
    "\n",
    "# Convert the sentences and tags to lists of indices\n",
    "train_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), \n",
    "               torch.tensor([tag_to_ix[tag] for tag in tags])) \n",
    "              for sentence, tags in zip(train_sentences, train_tags)]\n",
    "dev_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), \n",
    "             torch.tensor([tag_to_ix[tag] for tag in tags])) \n",
    "            for sentence, tags in zip(dev_sentences, dev_tags)]\n",
    "\n",
    "test_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), None) \n",
    "            for sentence in test_sentences]\n",
    "# Create the model\n",
    "model = BLSTM(len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# # Train the model\n",
    "# train(model, optimizer, loss_function, train_data, dev_data)\n",
    "\n",
    "# def predict(model, data, word_to_ix, tag_to_ix, file_path):\n",
    "#     with open(file_path, \"w\") as f:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for i, (sentence, tags) in enumerate(data):\n",
    "#                 sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "#                 tag_scores = model(sentence)\n",
    "#                 _, predicted_tags = torch.max(tag_scores, 1)\n",
    "#                 for j, word_index in enumerate(sentence):\n",
    "#                     word = list(word_to_ix.keys())[list(word_to_ix.values()).index(word_index.item())]\n",
    "#                     gold_tag = ''\n",
    "#                     if tags is not None:\n",
    "#                         gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "#                     pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "#                     f.write(str(j+1) + \" \" + word + \" \" + gold_tag + \" \" + pred_tag + \"\\n\")\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "def predict(model, original_sentences, data, word_to_ix, tag_to_ix, file_path, include_gold_tag=True):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (sentence, tags) in enumerate(data):\n",
    "                original_sentence = original_sentences[i]\n",
    "                sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "                tag_scores = model(sentence)\n",
    "                _, predicted_tags = torch.max(tag_scores, 1)\n",
    "                for j, word in enumerate(original_sentence):\n",
    "                    gold_tag = ''\n",
    "                    if tags is not None and include_gold_tag:\n",
    "                        gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "                    pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "                    f.write(str(j+1) + \" \" + word + \" \" + gold_tag + \" \" + pred_tag + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "if pred_file_path is not None:\n",
    "    with open(pred_file_path, \"a\") as f_pred:\n",
    "        for i, (sentence, tags) in enumerate(dev_data):\n",
    "            sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "            tag_scores = model(sentence)\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            for j, word_index in enumerate(sentence):\n",
    "                word = list(word_to_ix.keys())[list(word_to_ix.values()).index(word_index.item())]\n",
    "                gold_tag = ''\n",
    "                if tags is not None:\n",
    "                    gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "                pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "                f_pred.write(str(j+1) + \" \" + word + \" \" + \" \" + pred_tag + \"\\n\")\n",
    "            f_pred.write(\"\\n\")\n",
    "                \n",
    "pred_file_path = \"dev1.out\"\n",
    "perl_dev_file_path = \"perl_dev1.out\"\n",
    "test_pred_file_path = \"test1.out\"\n",
    "\n",
    "# Generate the predictions and write them to the prediction files\n",
    "predict(model, dev_sentences, dev_data, word_to_ix, tag_to_ix, pred_file_path, include_gold_tag=False)\n",
    "predict(model, dev_sentences, dev_data, word_to_ix, tag_to_ix, perl_dev_file_path, include_gold_tag=True)\n",
    "predict(model, test_sentences, test_data, word_to_ix, tag_to_ix, test_pred_file_path, include_gold_tag=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53440f8a",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5bb880cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4962.93867, Precision: 0.066, Recall: 0.058, F1 Score: 0.061\n",
      "Epoch 2: Train Loss: 3057.27422, Precision: 0.066, Recall: 0.059, F1 Score: 0.062\n",
      "Epoch 3: Train Loss: 2504.55590, Precision: 0.066, Recall: 0.060, F1 Score: 0.063\n",
      "Epoch 4: Train Loss: 2121.24326, Precision: 0.066, Recall: 0.060, F1 Score: 0.063\n",
      "Epoch 5: Train Loss: 1805.53576, Precision: 0.066, Recall: 0.060, F1 Score: 0.063\n",
      "Epoch 6: Train Loss: 1536.03511, Precision: 0.066, Recall: 0.060, F1 Score: 0.063\n",
      "Epoch 7: Train Loss: 1307.10273, Precision: 0.066, Recall: 0.061, F1 Score: 0.063\n",
      "Epoch 8: Train Loss: 1107.94243, Precision: 0.066, Recall: 0.061, F1 Score: 0.063\n",
      "Epoch 9: Train Loss: 940.60302, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 10: Train Loss: 790.19373, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 11: Train Loss: 684.41277, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 12: Train Loss: 568.95595, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 13: Train Loss: 501.69321, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 14: Train Loss: 457.31131, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 15: Train Loss: 384.11913, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 16: Train Loss: 646.10191, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 17: Train Loss: 446.86196, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 18: Train Loss: 268.52123, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 19: Train Loss: 202.91190, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 20: Train Loss: 156.93455, Precision: 0.067, Recall: 0.062, F1 Score: 0.065\n",
      "Epoch 21: Train Loss: 123.86288, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 22: Train Loss: 95.70102, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 23: Train Loss: 84.64367, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 24: Train Loss: 95.63939, Precision: 0.067, Recall: 0.062, F1 Score: 0.065\n",
      "Epoch 25: Train Loss: 167.91288, Precision: 0.067, Recall: 0.061, F1 Score: 0.064\n",
      "Epoch 26: Train Loss: 141.11279, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 27: Train Loss: 70.35410, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 28: Train Loss: 62.74675, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 29: Train Loss: 64.70193, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 30: Train Loss: 65.19701, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 31: Train Loss: 77.15865, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 32: Train Loss: 43.94508, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 33: Train Loss: 32.86790, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 34: Train Loss: 21.98562, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 35: Train Loss: 17.29272, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 36: Train Loss: 12.84268, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 37: Train Loss: 8.31554, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 38: Train Loss: 6.97020, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 39: Train Loss: 6.02890, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 40: Train Loss: 5.46545, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 41: Train Loss: 5.07373, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 42: Train Loss: 4.74313, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 43: Train Loss: 4.45617, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 44: Train Loss: 4.19676, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 45: Train Loss: 3.94591, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 46: Train Loss: 3.66944, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 47: Train Loss: 3.21350, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 48: Train Loss: 2.63572, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 49: Train Loss: 2.48988, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n",
      "Epoch 50: Train Loss: 2.39691, Precision: 0.067, Recall: 0.062, F1 Score: 0.064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gzip\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.33\n",
    "OUTPUT_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_path, word_to_ix, embed_dim):\n",
    "    embeddings = {}\n",
    "    with gzip.open(glove_path, 'rt', encoding='utf-8') as f:  # Use gzip.open instead of open\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = [float(val) for val in values[1:]]\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    embedding_matrix = torch.zeros(len(word_to_ix), embed_dim)\n",
    "    for word, index in word_to_ix.items():\n",
    "        if word in embeddings:\n",
    "            embedding_matrix[index] = torch.FloatTensor(embeddings[word])\n",
    "        else:\n",
    "            embedding_matrix[index] = torch.FloatTensor(embed_dim).uniform_(-0.5, 0.5)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class BLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tagset_size, pretrained_embeddings):\n",
    "        super(BLSTM, self).__init__()\n",
    "        self.embedding_dim = EMBEDDING_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.dropout = DROPOUT\n",
    "        self.output_dim = OUTPUT_DIM\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.blstm = nn.LSTM(self.embedding_dim, self.hidden_dim, \n",
    "                             num_layers=self.num_layers, bidirectional=True,\n",
    "                             dropout=self.dropout)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim*2, self.output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.tag_projection = nn.Linear(self.output_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.blstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        elu_out = self.elu(tag_space)\n",
    "        tag_scores = self.tag_projection(elu_out)\n",
    "        return tag_scores\n",
    "    \n",
    "# Define the training function\n",
    "def train(model, optimizer, loss_function, train_data, dev_data):\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        model.train()\n",
    "        for sentence, tags in train_data:\n",
    "            model.zero_grad()\n",
    "            sentence = autograd.Variable(torch.LongTensor(sentence))\n",
    "            targets = autograd.Variable(torch.LongTensor(tags))\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            num_correct += (predicted_tags == targets).sum().item()\n",
    "            num_total += len(targets)\n",
    "\n",
    "        precision, recall, f1 = evaluate(model, dev_data)\n",
    "        print(\"Epoch {}: Train Loss: {:.5f}, Precision: {:.3f}, Recall: {:.3f}, F1 Score: {:.3f}\" .format(epoch+1, train_loss, precision, recall, f1))\n",
    "\n",
    "torch.save(model.state_dict(), 'blstm2.pt')\n",
    "\n",
    "def evaluate(model, data):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in data:\n",
    "            sentence = autograd.Variable(torch.LongTensor(sentence))\n",
    "            targets = autograd.Variable(torch.LongTensor(tags))\n",
    "            tag_scores = model(sentence)\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            num_correct += (predicted_tags == targets).sum().item()\n",
    "            num_total += len(targets)\n",
    "            correct += (predicted_tags == targets).sum().item() * 1.0 / len(targets)\n",
    "\n",
    "    precision = correct / num_correct\n",
    "    recall = correct / num_total\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Read in the train and dev data\n",
    "def read_data(file_path, has_tags=True):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(file_path) as f:\n",
    "        sentence = []\n",
    "        tags_sequence = []\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tags_sequence)\n",
    "                sentence = []\n",
    "                tags_sequence = []\n",
    "            else:\n",
    "                line = line.strip().split()\n",
    "                word = line[1]\n",
    "                sentence.append(word)\n",
    "                if has_tags:\n",
    "                    tag = line[2]\n",
    "                    tags_sequence.append(tag)\n",
    "    return sentences, tags\n",
    "\n",
    "\n",
    "glove_path = \"glove.6B.100d.gz\"\n",
    "pretrained_embeddings = load_glove_embeddings(glove_path, word_to_ix, EMBEDDING_DIM)\n",
    "\n",
    "# Define the paths to the train and dev data files\n",
    "train_file_path = \"train.txt\"\n",
    "dev_file_path = \"dev.txt\"\n",
    "test_file_path = \"test.txt\"\n",
    "\n",
    "\n",
    "# Read in the train and dev data\n",
    "train_sentences, train_tags = read_data(train_file_path)\n",
    "dev_sentences, dev_tags = read_data(dev_file_path)\n",
    "test_sentences, _ = read_data(test_file_path, has_tags=False)\n",
    "\n",
    "# Create the vocabulary and tag sets\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "for sentence, tags in zip(train_sentences, train_tags):\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# Add the unknown token to the vocabulary\n",
    "word_to_ix[\"<UNK>\"] = len(word_to_ix)\n",
    "\n",
    "# Convert the sentences and tags to lists of indices\n",
    "train_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), \n",
    "               torch.tensor([tag_to_ix[tag] for tag in tags])) \n",
    "              for sentence, tags in zip(train_sentences, train_tags)]\n",
    "dev_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), \n",
    "             torch.tensor([tag_to_ix[tag] for tag in tags])) \n",
    "            for sentence, tags in zip(dev_sentences, dev_tags)]\n",
    "\n",
    "test_data = [(torch.tensor([word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence]), None) \n",
    "            for sentence in test_sentences]\n",
    "# Create the model\n",
    "model = BLSTM(len(word_to_ix), len(tag_to_ix), pretrained_embeddings)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train(model, optimizer, loss_function, train_data, dev_data)\n",
    "\n",
    "# def predict(model, data, word_to_ix, tag_to_ix, file_path):\n",
    "#     with open(file_path, \"w\") as f:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for i, (sentence, tags) in enumerate(data):\n",
    "#                 sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "#                 tag_scores = model(sentence)\n",
    "#                 _, predicted_tags = torch.max(tag_scores, 1)\n",
    "#                 for j, word_index in enumerate(sentence):\n",
    "#                     word = list(word_to_ix.keys())[list(word_to_ix.values()).index(word_index.item())]\n",
    "#                     gold_tag = ''\n",
    "#                     if tags is not None:\n",
    "#                         gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "#                     pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "#                     f.write(str(j+1) + \" \" + word + \" \" + gold_tag + \" \" + pred_tag + \"\\n\")\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "def predict(model, original_sentences, data, word_to_ix, tag_to_ix, file_path, include_gold_tag=True):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (sentence, tags) in enumerate(data):\n",
    "                original_sentence = original_sentences[i]\n",
    "                sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "                tag_scores = model(sentence)\n",
    "                _, predicted_tags = torch.max(tag_scores, 1)\n",
    "                for j, word in enumerate(original_sentence):\n",
    "                    gold_tag = ''\n",
    "                    if tags is not None and include_gold_tag:\n",
    "                        gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "                    pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "                    f.write(str(j+1) + \" \" + word + \" \" + gold_tag + \" \" + pred_tag + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "if pred_file_path is not None:\n",
    "    with open(pred_file_path, \"a\") as f_pred:\n",
    "        for i, (sentence, tags) in enumerate(dev_data):\n",
    "            sentence = autograd.Variable(sentence.view(-1, 1))\n",
    "            tag_scores = model(sentence)\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            for j, word_index in enumerate(sentence):\n",
    "                word = list(word_to_ix.keys())[list(word_to_ix.values()).index(word_index.item())]\n",
    "                gold_tag = ''\n",
    "                if tags is not None:\n",
    "                    gold_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(tags[j].item())]\n",
    "                pred_tag = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(predicted_tags[j].item())]\n",
    "                f_pred.write(str(j+1) + \" \" + word + \" \" + \" \" + pred_tag + \"\\n\")\n",
    "            f_pred.write(\"\\n\")\n",
    "                \n",
    "pred_file_path = \"dev2.out\"\n",
    "perl_dev_file_path = \"perl_dev2.out\"\n",
    "test_pred_file_path = \"test2.out\"\n",
    "\n",
    "# Generate the predictions and write them to the prediction files\n",
    "predict(model, dev_sentences, dev_data, word_to_ix, tag_to_ix, pred_file_path, include_gold_tag=False)\n",
    "predict(model, dev_sentences, dev_data, word_to_ix, tag_to_ix, perl_dev_file_path, include_gold_tag=True)\n",
    "predict(model, test_sentences, test_data, word_to_ix, tag_to_ix, test_pred_file_path, include_gold_tag=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f3e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e469cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4d559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d19795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "717123f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM(\n",
       "  (word_embeddings): Embedding(23625, 100)\n",
       "  (blstm): LSTM(100, 256, dropout=0.33, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (tag_projection): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create a new instance of the model with the same architecture\n",
    "loaded_model = BLSTM(len(word_to_ix), len(tag_to_ix), pretrained_embeddings)\n",
    "\n",
    "# Load the saved state dictionary into the new model\n",
    "loaded_model.load_state_dict(torch.load('blstm2.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b1f9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_file_path = \"dev_predictions.txt\"\n",
    "test_pred_file_path = \"test_predictions.txt\"\n",
    "\n",
    "# Generate the predictions and write them to the prediction files\n",
    "predict(loaded_model, dev_sentences, dev_data, word_to_ix, tag_to_ix, dev_pred_file_path)\n",
    "predict(loaded_model, test_sentences, test_data, word_to_ix, tag_to_ix, test_pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "517ca823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM(\n",
       "  (word_embeddings): Embedding(23625, 100)\n",
       "  (blstm): LSTM(100, 256, dropout=0.33, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (tag_projection): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create a new instance of the model with the same architecture\n",
    "loaded_model = BLSTM(len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Load the saved state dictionary into the new model\n",
    "loaded_model.load_state_dict(torch.load('blstm1.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model_lstm.state_dict(), 'blstm1.pt')\n",
    "loaded_model_lstm = LSTM(vocab_size = len(vocab)+1, embedding_dim = 100, lstm_hidden_size = 256, fc_num_neurons = 128, num_classes = len(labels)).to(device)\n",
    "loaded_model_lstm.load_state_dict(torch.load('blstm1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68e079e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_file_path = \"dev_predictions1.txt\"\n",
    "test_pred_file_path = \"test_predictions1.txt\"\n",
    "\n",
    "# Generate the predictions and write them to the prediction files\n",
    "predict(loaded_model, dev_sentences, dev_data, word_to_ix, tag_to_ix, dev_pred_file_path)\n",
    "predict(loaded_model, test_sentences, test_data, word_to_ix, tag_to_ix, test_pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5820584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23625\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4daaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
